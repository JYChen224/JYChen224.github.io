
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>JYChen的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="究天人之际，通古今之变，成一家之言">
<meta property="og:type" content="website">
<meta property="og:title" content="JYChen的博客">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;index.html">
<meta property="og:site_name" content="JYChen的博客">
<meta property="og:description" content="究天人之际，通古今之变，成一家之言">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="JYChen">
<meta name="twitter:card" content="summary">
  
    <link rel="alternative" href="/atom.xml" title="JYChen的博客" type="application/atom+xml">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 4.1.1"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        
          <a id="nav-github" class="nav-icon" href="https://github.com/myname" target="_blank" rel="noopener"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">My Blog</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">MyLogo</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
        <a class="main-nav-link" href="/about">About</a>
      
      <a class="main-nav-link st-search-show-outputs">搜索</a>
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-TRPO" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2020/01/06/TRPO/" class="article-date">
  <time datetime="2020-01-06T13:15:16.394Z" itemprop="datePublished">2020-01-06</time>
</h3>
    
  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/06/TRPO/">TRPO</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="TRPO-Trust-Region-Policy-Optimization"><a href="#TRPO-Trust-Region-Policy-Optimization" class="headerlink" title="TRPO-Trust Region Policy Optimization"></a>TRPO-Trust Region Policy Optimization</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>在策略梯度算法中，参数更新的表达式为：</p>
<script type="math/tex; mode=display">\theta_{new} = \theta_{old} + \alpha\nabla_{\theta} J</script><p>在强化学习中使用梯度下降方法进行参数更新时，由于数据集是在交互的过程中不断更新的，因此会遇到如下两个问题：</p>
<ul>
<li>梯度估计不准确</li>
<li>步长参数不容易确定</li>
</ul>
<p>尤其当算法采用不合适的步长时，参数会更新到一个不好的策略，从而采集到的更差的样本，估计出更差的梯度，更新到更更不好的策略，如此循环，使得策略没办法提升。TRPO方法在进行梯度下降的同时，对步长加以限制，并可以自动的调整步长，在理论上确保策略的提升是非减的，这样每次策略更新的时候我们都能有信心新的策略比旧的策略要好。</p>
<h3 id="回报函数与优势函数"><a href="#回报函数与优势函数" class="headerlink" title="回报函数与优势函数"></a>回报函数与优势函数</h3><p>我们用$ \pi $来表示策略，用$\eta(\pi)$来表示策略的回报函数，即期望折扣回报：</p>
<script type="math/tex; mode=display">\eta(\pi)=E_{s_0,a_0,\dots}\left[\sum_{t=0}^{\infty}\gamma^tr(s_t)\right]\quad\quad(1)</script><p>其中$s_{0} \sim \rho_{0}\left(s_{0}\right), a_{t} \sim \pi\left(a_{t} | s_{t}\right), s_{t+1} \sim P\left(s_{t+1} | s_{t}, a_{t}\right)$。</p>
<p>然后我们定义优势函数$A_\pi(s,a)$:</p>
<script type="math/tex; mode=display">
Q_{\pi}\left(s_{t}, a_{t}\right)=\mathbb{E}_{s_{t+1}, a_{t+1}, \ldots}\left[\sum_{l=0}^{\infty} \gamma^{l} r\left(s_{t+l}\right)\right]</script><script type="math/tex; mode=display">
V_{\pi}\left(s_{t}\right)=\mathbb{E}_{a_{t}, s_{t+1}, \ldots}\left[\sum_{l=0}^{\infty} \gamma^{l} r\left(s_{t+l}\right)\right]</script><script type="math/tex; mode=display">
A_{\pi}(s, a)=Q_{\pi}(s, a)-V_{\pi}(s)\quad\quad(2)</script><p>其中$a_{t} \sim \pi\left(a_{t} | s_{t}\right), s_{t+1} \sim P\left(s_{t+1} | s_{t}, a_{t}\right)$。<br>优势函数描述了动作值函数与状态值函数之间的差异，相当于Q_Learning中的$\delta$，后面可以看到，使用优势函数可以描述两个策略之间回报函数的差异。</p>
<h3 id="回报函数的分解"><a href="#回报函数的分解" class="headerlink" title="回报函数的分解"></a>回报函数的分解</h3><script type="math/tex; mode=display">
\eta(\tilde{\pi})=\eta(\pi)+\mathbb{E}_{s_{0}, a_{0}, \cdots \sim \tilde{\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t} A_{\pi}\left(s_{t}, a_{t}\right)\right]\quad\quad(3)</script><p>证明：</p>
<script type="math/tex; mode=display">\begin{aligned}
& \mathbb{E}_{\tau | \tilde{\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t} A_{\pi}\left(s_{t}, a_{t}\right)\right]
\\&=\mathbb{E}_{\tau | \tilde{\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t}\left(r\left(s_{t}\right)+\gamma V_{\pi}\left(s_{t+1}\right)-V_{\pi}\left(s_{t}\right)\right)\right]
\\ &=\mathbb{E}_{\tau | \tilde{\pi}}\left[-V_{\pi}\left(s_{0}\right)+\sum_{t=0}^{\infty} \gamma^{t} r\left(s_{t}\right)\right]
\\ &=-\mathbb{E}_{s_{0}}\left[V_{\pi}\left(s_{0}\right)\right]+\mathbb{E}_{\tau | \tilde{\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t} r\left(s_{t}\right)\right]
\\ &=-\eta(\pi)+\eta(\tilde{\pi})
\end{aligned}</script><p>公式的第二行是优势函数的展开表达，第三行是通过错位相消化简了$V_{\pi}\left(s\right)$,第五行是根据回报函数的定义得到，这里注意到状态值函数与回报函数存在关系：$\eta(\pi)=E_{s_0}\left[V_{\pi}\left(s_{0}\right)\right]$</p>
<p>我们将公式（3）进行进一步的化简：</p>
<script type="math/tex; mode=display">\begin{aligned} \eta(\tilde{\pi}) &=\eta(\pi)+\sum_{t=0}^{\infty} \sum_{s} P\left(s_{t}=s | \tilde{\pi}\right) \sum_{a} \tilde{\pi}(a | s) \gamma^{t} A_{\pi}(s, a) \\ &=\eta(\pi)+\sum_{s} \sum_{t=0}^{\infty} \gamma^{t} P\left(s_{t}=s | \tilde{\pi}\right) \sum_{a} \tilde{\pi}(a | s) A_{\pi}(s, a) \\ &=\eta(\pi)+\sum_{s} \rho_{\tilde{\pi}}(s) \sum_{a} \tilde{\pi}(a | s) A_{\pi}(s, a) \end{aligned}</script><p>其中$\rho_{\pi}(s)=P\left(s_{0}=s\right)+\gamma P\left(s_{1}=s\right)+\gamma^{2} P\left(s_{2}=s\right)+\ldots$，表示折扣访问频率，可以理解为在策略$\tilde{\pi}$下，状态的分布。</p>
<p>从公式可以看出，在从一个策略优化到新的策略的时候，只要保证每个状态$S$下，保持一个非负的advantage，即$\sum_{a} \tilde{\pi}(a | s) A_{\pi}(s, a)&gt;0$，则可以保证新的策略肯定是更好的，这样通过迭代算法可以收敛到最优的策略。</p>
<h3 id="回报函数的近似"><a href="#回报函数的近似" class="headerlink" title="回报函数的近似"></a>回报函数的近似</h3><p>$\rho_{\tilde{\pi}}(s)$描述了在策略$\tilde{\pi}$下，状态的分布。此时状态s的分布由新的策略产生，对新的策略严重依赖，使得优化问题非常复杂，因此TRPO采用$\rho_{\pi}(s)$对$\rho_{\tilde{\pi}}(s)$进行近似，从而得到新的目标函数：</p>
<script type="math/tex; mode=display">L_{\pi}(\tilde{\pi})=\eta(\pi)+\sum_{s} \rho_{\pi}(s) \sum_{a} \tilde{\pi}(a | s) A_{\pi}(s, a)\quad\quad(4)</script><p>近似目标函数与原函数之间存在如下关系：</p>
<script type="math/tex; mode=display">
\begin{aligned} L_{\pi_{\theta_{0}}}\left(\pi_{\theta_{0}}\right) &=\eta\left(\pi_{\theta_{0}}\right) \\\left.\nabla_{\theta} L_{\pi_{\theta_{0}}}\left(\pi_{\theta}\right)\right|_{\theta=\theta_{0}} &=\left.\nabla_{\theta} \eta\left(\pi_{\theta}\right)\right|_{\theta=\theta_{0}} \end{aligned}</script><p>即$L_{\pi}(\tilde{\pi})$是对$\eta(\tilde{\pi})$的一阶等价近似，证明如下：</p>
<p>对于第一个等式</p>
<script type="math/tex; mode=display">
\begin{aligned} L_{\pi_{\theta_{0}}}\left(\pi_{\theta_{0}}\right) &=\eta\left(\pi_{\theta_{0}}\right) + \sum_{s} \rho_{\pi_{\theta_{0}}}(s) \sum_{a} \pi_{\theta_{0}}(a | s) A_{\pi_{\theta_{0}}}(s, a) \\ \end{aligned}</script><p>由于同一个策略对自身优势函数的期望为零，$\sum_{a} \pi_{\theta_{0}}(a | s) A_{\pi_{\theta_{0}}}(s, a)=0$，因此第一个等式成立。</p>
<p>对于第二个等式</p>
<script type="math/tex; mode=display">\left.\nabla_{\theta} L_{\pi_{\theta_{0}}}\left(\pi_{\theta}\right)\right|_{\theta=\theta_{0}} =\sum_{s} \rho_{\pi}(s) \sum_{a} \nabla_{\theta}{\pi}(a | s) A_{\pi}(s, a)|_{\theta=\theta_{0}}</script><script type="math/tex; mode=display">\nabla_{\theta} \eta\left(\pi_{\theta}\right)|_{\theta=\theta_{0}} = \sum_{s} \nabla_{\theta}\rho_{\pi}(s) \sum_{a}{\pi}(a | s) A_{\pi}(s, a)|_{\theta=\theta_{0}} + \sum_{s} \rho_{\pi}(s) \sum_{a} \nabla_{\theta}{\pi}(a | s) A_{\pi}(s, a)|_{\theta=\theta_{0}}</script><p>由于$\sum_{a}{\pi}(a | s) A_{\pi}(s, a)|_{\theta=\theta_{0}}=0$，因此第二个等式成立。</p>
<p>进一步，当策略更新满足如$\pi_{\text {new }}(a | s)=(1-\alpha) \pi_{\text {old }}(a | s)+\alpha \pi^{\prime}(a | s)$时，近似函数与原目标函数存在如下关系：</p>
<script type="math/tex; mode=display">
\eta\left(\pi_{\text {new }}\right) \geq L_{\pi_{\text {old }}}\left(\pi_{\text {new }}\right)-\frac{2 \epsilon \gamma}{(1-\gamma)^{2}} \alpha^{2}</script><p>或者用KL散度表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned} \eta(\tilde{\pi}) & \geq L_{\pi}(\tilde{\pi})-C D_{\mathrm{KL}}^{\max }(\pi, \tilde{\pi})  \\ & \text { where } C=\frac{2 \epsilon \gamma}{(1-\gamma)^{2}} \end{aligned}   \quad\quad\quad\quad (5)</script><p>公式(5)提供了$\eta(\tilde{\pi})$的下界，我们令$M_i(\pi)=L_{\pi_{i}}(\pi)-C D_{\mathrm{KL}}^{\max }\left(\pi_{i}, \pi\right)$，则有：</p>
<script type="math/tex; mode=display">
\begin{array}{l}{\eta\left(\pi_{i+1}\right) \geq M_{i}\left(\pi_{i+1}\right)} \\ {\eta\left(\pi_{i}\right)=M_{i}\left(\pi_{i}\right)} \\ {\eta\left(\pi_{i+1}\right)-\eta\left(\pi_{i}\right) \geq M_{i}\left(\pi_{i+1}\right)-M\left(\pi_{i}\right)}\end{array}</script><p>只要每次选择策略使得$M_i&gt;0$，则这样的策略就是更好的策略，算法就可以收敛到最优策略。</p>
<h3 id="优化问题的简化"><a href="#优化问题的简化" class="headerlink" title="优化问题的简化"></a>优化问题的简化</h3><p>从过前面的分析，我们把求解回报函数$\eta(\pi)$最大的问题，转化为求$\eta(\pi)$的下界函数$L_{\pi_{i}}(\pi)-C D_{\mathrm{KL}}^{\max }\left(\pi_{i}, \pi\right)$最大的问题，形式化为：</p>
<script type="math/tex; mode=display">
\underset{\theta}{\operatorname{maximize}}\left[L_{\theta_{\mathrm{old}}}(\theta)-C D_{\mathrm{KL}}^{\max }\left(\theta_{\mathrm{old}}, \theta\right)\right]</script><p>这个优化可以看做是在目标函数$L$的基础上，增加了一个惩罚项，即在最大化$L$的基础上，限制每次优化的参数变化程度，其中$C$为惩罚因子，KL散度表示参数之间的距离。但如果固定惩罚因子$C$，则每次迭代的步长很小，因此希望自动调整$C$的取值，这样优化问题可以转化为带约束优化的形式：</p>
<script type="math/tex; mode=display">
\begin{array}{l}{\underset{\theta}{\operatorname{maximize}} L_{\theta_{\text {old }}}(\theta)} \\ {\text { subject to } \bar{D}_{\mathrm{KL}}^{\rho_{\theta_{\text {old }}}}\left(\theta_{\text {old }}, \theta\right) \leq \delta}\end{array} \quad\quad\quad\quad (6)</script><p>进一步，我们将目标函数一阶逼近，在$\theta_{\text {old }}$进行一阶泰勒展开：</p>
<script type="math/tex; mode=display">L_{\theta_{\text {old }}}(\theta)\doteq \left.\nabla_{\theta} L_{\theta_{\text {old }}}(\theta)\right|_{\theta=\theta_{\text {old }}} \cdot\left(\theta-\theta_{\text {old }}\right)</script><p>将约束条件进行二阶逼近，在$\theta_{\text {old }}$进行二阶泰勒展开</p>
<script type="math/tex; mode=display">
\begin{aligned}
& D_{KL}(\pi_{\theta_{old}}(\cdot|s)||\pi_{\theta}(\cdot|s)) \\
& = E_{x\sim\pi_{old}}log\pi_{\theta_{old}} - E_{x\sim\pi_{old}}log\pi_{\theta} \\
& \approx E_{\theta_{old}}[log\pi_{\theta_{old}}] - (E_{\theta_{old}}[log\pi_{\theta_{old}}] + E_{\theta_{old}}[\nabla log\pi_{\theta_{old}}]\Delta\theta+\frac{1}{2}\Delta\theta^TE_{\theta_{old}}[\nabla^2 log\pi_{\theta_{old}}]\Delta\theta)\\
& = -\frac{1}{2}\Delta\theta^TE_{\theta_{old}}[\nabla^2 log\pi_{\theta_{old}}]\Delta\theta
\end{aligned}</script><p>其中最后一步的等式由于$log\pi_{\theta_{old}}$为极点，$\nabla log\pi_{\theta_{old}}=0$。</p>
<p>令$A = E_{\theta_{old}}[\nabla^2 log\pi_{\theta_{old}}]$，则优化问题最终可描述为：</p>
<script type="math/tex; mode=display">
\begin{array}{l}{\underset{\theta}{\operatorname{maximize}}\left[\left.\nabla_{\theta} L_{\theta_{\text {old }}}(\theta)\right|_{\theta=\theta_{\text {old }}} \cdot\left(\theta-\theta_{\text {old }}\right)\right]} \\ {\text { subject to } \frac{1}{2}\left(\theta_{\text {old }}-\theta\right)^{T} A\left(\theta_{\text {old }}\right)\left(\theta_{\text {old }}-\theta\right) \leq \delta}\end{array}</script><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>在梯度上升法中，我们判断最陡峭的方向，之后朝着那个方向前进。但是如果学习率太高的话，这样的优化也许会远离真正的最优策略。在TRPO中，我们用 δ 变量来限制我们的搜索区域，这样的区域可以保证在它达到局部或者全局最优策略之前，它的优化策略将会优于当前策略，从而使算法稳定的收敛到一个最优策略。</p>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://yoursite.com/2020/01/06/TRPO/" data-id="ck52j8r3o0001houxatafazw6" class="article-share-link">分享到</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-DDPG" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2020/01/06/DDPG/" class="article-date">
  <time datetime="2020-01-06T13:15:16.388Z" itemprop="datePublished">2020-01-06</time>
</h3>
    
  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/06/DDPG/">DDPG</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h3 id="强化学习中逼近函数的相容性问题"><a href="#强化学习中逼近函数的相容性问题" class="headerlink" title="强化学习中逼近函数的相容性问题"></a>强化学习中逼近函数的相容性问题</h3><hr>
<h4 id="随机性策略的逼近函数相容性"><a href="#随机性策略的逼近函数相容性" class="headerlink" title="随机性策略的逼近函数相容性"></a>随机性策略的逼近函数相容性</h4><p>对于随机性策略，策略更新的梯度计算公式为：</p>
<script type="math/tex; mode=display">\nabla_\theta J=E[Q^\pi(s,a)\nabla log_\theta\pi_\theta(a|s)]</script><p>随机性策略逼近函数的相容性条件为：</p>
<ul>
<li>[1] $Q^w=\nabla log_\theta\pi_\theta(a|s)^Tw$ </li>
<li>[2] 目标函数为 $\epsilon^2=E[Q_w(s,a)-Q^\pi(s,a)]^2$</li>
</ul>
<p>证明：<br>由于$w$为目标函数的极值，因此有</p>
<script type="math/tex; mode=display">\frac{\partial\epsilon^2}{\partial w} = E[Q_w(s,a)-Q^\pi(s,a)]\frac{\partial Q_w(s,a)}{\partial w} = 0</script><p>将条件[1]带入上式得：</p>
<script type="math/tex; mode=display">E[Q_w(s,a)-Q^\pi(s,a)]\nabla log_\theta\pi_\theta(a|s) = 0</script><p>展开得：</p>
<script type="math/tex; mode=display">E[Q_w(s,a)\nabla log_\theta\pi_\theta(a|s)] = E[Q^\pi(s,a)\nabla log_\theta\pi_\theta(a|s)]</script><p>即可得到使用逼近函数进行梯度估计的无偏性</p>
<hr>
<h4 id="确定性策略的逼近函数相容性"><a href="#确定性策略的逼近函数相容性" class="headerlink" title="确定性策略的逼近函数相容性"></a>确定性策略的逼近函数相容性</h4><p>对于确定性策略，策略更新的梯度计算公式为：</p>
<script type="math/tex; mode=display">\nabla_\theta J=E[\nabla_\theta \mu_\theta(s)\nabla_a Q^\mu(s,a)\big|_{a=\mu(s)}]</script><p>确定性策略逼近函数的相容性条件为：</p>
<ul>
<li>[1] $\nabla_a Q^w(s,a)\big|_{a=\mu(s)}=\nabla_\theta\mu_\theta(s)^Tw$</li>
<li>[2] 目标函数为 $MSE(\theta,w)=E[\epsilon(s;\theta,w)^T\epsilon(s;\theta,w)]$<br>其中 $\epsilon(s;\theta,w) = (\nabla_a Q^w - \nabla_a Q^\mu)$</li>
</ul>
<p>证明：<br>由于$w$为目标函数的极值，因此有</p>
<script type="math/tex; mode=display">(\nabla_a Q^w - \nabla_a Q^\mu)\frac{\partial\nabla_a Q^w}{\partial w} = 0</script><p>带入条件[1]得</p>
<script type="math/tex; mode=display">(\nabla_a Q^w - \nabla_a Q^\mu)\nabla_\theta\mu_\theta(s) = 0</script><p>展开有</p>
<script type="math/tex; mode=display">\nabla_a Q^w\nabla_\theta\mu_\theta(s) = \nabla_a Q^\mu\nabla_\theta\mu_\theta(s)</script><p>即可得到使用逼近函数进行梯度估计的无偏性</p>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://yoursite.com/2020/01/06/DDPG/" data-id="ck52j8r430007houxe6pdfiu2" class="article-share-link">分享到</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-W7" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2020/01/06/W7/" class="article-date">
  <time datetime="2020-01-06T13:15:16.382Z" itemprop="datePublished">2020-01-06</time>
</h3>
    
  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/06/W7/">W7</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="TRPO-Trust-Region-Policy-Optimization"><a href="#TRPO-Trust-Region-Policy-Optimization" class="headerlink" title="TRPO-Trust Region Policy Optimization"></a>TRPO-Trust Region Policy Optimization</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>在强化学习的策略梯度算法中，参数更新的表达式为：</p>
<script type="math/tex; mode=display">\theta_{new} = \theta_{old} + \alpha\nabla_{\theta} J</script><p>在强化学习中使用梯度下降方法进行参数更新时，由于数据集是在交互的过程中不断更新的，因此会遇到如下两个问题：</p>
<ul>
<li>梯度估计不准确</li>
<li>步长参数不容易确定</li>
</ul>
<p>尤其当算法采用不合适的步长时，参数会更新到一个不好的策略，从而采集到的更差的样本，估计出更差的梯度，更新到更更不好的策略，如此循环，使得策略没办法提升。TRPO方法在进行梯度下降的同时，对步长加以限制，并可以自动的调整步长，在理论上确保策略的提升是非减的，这样每次策略更新的时候我们都能有信心新的策略比旧的策略要好。</p>
<h3 id="回报函数与优势函数"><a href="#回报函数与优势函数" class="headerlink" title="回报函数与优势函数"></a>回报函数与优势函数</h3><p>我们用$\pi$来表示策略，用$\eta(\pi)$来表示策略的回报函数，即期望折扣回报：</p>
<script type="math/tex; mode=display">\eta(\pi)=E_{s_0,a_0,\dots}\left[\sum_{t=0}^{\infty}\gamma^tr(s_t)\right]\quad\quad(1)</script><p>其中$s_{0} \sim \rho_{0}\left(s_{0}\right), a_{t} \sim \pi\left(a_{t} | s_{t}\right), s_{t+1} \sim P\left(s_{t+1} | s_{t}, a_{t}\right)$。</p>
<p>然后我们定义优势函数$A_\pi(s,a)$:</p>
<script type="math/tex; mode=display">
Q_{\pi}\left(s_{t}, a_{t}\right)=\mathbb{E}_{s_{t+1}, a_{t+1}, \ldots}\left[\sum_{l=0}^{\infty} \gamma^{l} r\left(s_{t+l}\right)\right]</script><script type="math/tex; mode=display">
V_{\pi}\left(s_{t}\right)=\mathbb{E}_{a_{t}, s_{t+1}, \ldots}\left[\sum_{l=0}^{\infty} \gamma^{l} r\left(s_{t+l}\right)\right]</script><script type="math/tex; mode=display">
A_{\pi}(s, a)=Q_{\pi}(s, a)-V_{\pi}(s)\quad\quad(2)</script><p>其中$a_{t} \sim \pi\left(a_{t} | s_{t}\right), s_{t+1} \sim P\left(s_{t+1} | s_{t}, a_{t}\right)$。<br>优势函数描述了动作值函数与状态值函数之间的差异，相当于Q_Learning中的$\delta$，后面可以看到，使用优势函数可以描述两个策略之间回报函数的差异。</p>
<h3 id="回报函数的分解"><a href="#回报函数的分解" class="headerlink" title="回报函数的分解"></a>回报函数的分解</h3><script type="math/tex; mode=display">
\eta(\tilde{\pi})=\eta(\pi)+\mathbb{E}_{s_{0}, a_{0}, \cdots \sim \tilde{\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t} A_{\pi}\left(s_{t}, a_{t}\right)\right]\quad\quad(3)</script><p>证明：</p>
<script type="math/tex; mode=display">\begin{aligned}
& \mathbb{E}_{\tau | \tilde{\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t} A_{\pi}\left(s_{t}, a_{t}\right)\right]
\\&=\mathbb{E}_{\tau | \tilde{\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t}\left(r\left(s_{t}\right)+\gamma V_{\pi}\left(s_{t+1}\right)-V_{\pi}\left(s_{t}\right)\right)\right]
\\ &=\mathbb{E}_{\tau | \tilde{\pi}}\left[-V_{\pi}\left(s_{0}\right)+\sum_{t=0}^{\infty} \gamma^{t} r\left(s_{t}\right)\right]
\\ &=-\mathbb{E}_{s_{0}}\left[V_{\pi}\left(s_{0}\right)\right]+\mathbb{E}_{\tau | \tilde{\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t} r\left(s_{t}\right)\right]
\\ &=-\eta(\pi)+\eta(\tilde{\pi})
\end{aligned}</script><p>公式的第二行是优势函数的展开表达，第三行是通过错位相消化简了$V_{\pi}\left(s\right)$,第五行是根据回报函数的定义得到，这里注意到状态值函数与回报函数存在关系：$\eta(\pi)=E_{s_0}\left[V_{\pi}\left(s_{0}\right)\right]$</p>
<p>我们将公式（3）进行进一步的化简：</p>
<script type="math/tex; mode=display">\begin{aligned} \eta(\tilde{\pi}) &=\eta(\pi)+\sum_{t=0}^{\infty} \sum_{s} P\left(s_{t}=s | \tilde{\pi}\right) \sum_{a} \tilde{\pi}(a | s) \gamma^{t} A_{\pi}(s, a) \\ &=\eta(\pi)+\sum_{s} \sum_{t=0}^{\infty} \gamma^{t} P\left(s_{t}=s | \tilde{\pi}\right) \sum_{a} \tilde{\pi}(a | s) A_{\pi}(s, a) \\ &=\eta(\pi)+\sum_{s} \rho_{\tilde{\pi}}(s) \sum_{a} \tilde{\pi}(a | s) A_{\pi}(s, a) \end{aligned}</script><p>其中$\rho_{\pi}(s)=P\left(s_{0}=s\right)+\gamma P\left(s_{1}=s\right)+\gamma^{2} P\left(s_{2}=s\right)+\ldots$，表示折扣访问频率，可以理解为在策略$\tilde{\pi}$下，状态的分布。</p>
<p>从公式可以看出，在从一个策略优化到新的策略的时候，只要保证每个状态$S$下，保持一个非负的advantage，即$\sum_{a} \tilde{\pi}(a | s) A_{\pi}(s, a)&gt;0$，则可以保证新的策略肯定是更好的，这样通过迭代算法可以收敛到最优的策略。</p>
<h3 id="回报函数的近似"><a href="#回报函数的近似" class="headerlink" title="回报函数的近似"></a>回报函数的近似</h3><p>$\rho_{\tilde{\pi}}(s)$描述了在策略$\tilde{\pi}$下，状态的分布。此时状态s的分布由新的策略产生，对新的策略严重依赖，使得优化问题非常复杂，因此TRPO采用$\rho_{\pi}(s)$对$\rho_{\tilde{\pi}}(s)$进行近似，从而得到新的目标函数：</p>
<script type="math/tex; mode=display">L_{\pi}(\tilde{\pi})=\eta(\pi)+\sum_{s} \rho_{\pi}(s) \sum_{a} \tilde{\pi}(a | s) A_{\pi}(s, a)\quad\quad(4)</script><p>近似目标函数与原函数之间存在如下关系：</p>
<script type="math/tex; mode=display">
\begin{aligned} L_{\pi_{\theta_{0}}}\left(\pi_{\theta_{0}}\right) &=\eta\left(\pi_{\theta_{0}}\right) \\\left.\nabla_{\theta} L_{\pi_{\theta_{0}}}\left(\pi_{\theta}\right)\right|_{\theta=\theta_{0}} &=\left.\nabla_{\theta} \eta\left(\pi_{\theta}\right)\right|_{\theta=\theta_{0}} \end{aligned}</script><p>即$L_{\pi}(\tilde{\pi})$是对$\eta(\tilde{\pi})$的一阶等价近似，证明如下：</p>
<p>对于第一个等式</p>
<script type="math/tex; mode=display">
\begin{aligned} L_{\pi_{\theta_{0}}}\left(\pi_{\theta_{0}}\right) &=\eta\left(\pi_{\theta_{0}}\right) + \sum_{s} \rho_{\pi_{\theta_{0}}}(s) \sum_{a} \pi_{\theta_{0}}(a | s) A_{\pi_{\theta_{0}}}(s, a) \\ \end{aligned}</script><p>由于同一个策略对自身优势函数的期望为零，$\sum_{a} \pi_{\theta_{0}}(a | s) A_{\pi_{\theta_{0}}}(s, a)=0$，因此第一个等式成立。</p>
<p>对于第二个等式</p>
<script type="math/tex; mode=display">\left.\nabla_{\theta} L_{\pi_{\theta_{0}}}\left(\pi_{\theta}\right)\right|_{\theta=\theta_{0}} =\sum_{s} \rho_{\pi}(s) \sum_{a} \nabla_{\theta}{\pi}(a | s) A_{\pi}(s, a)|_{\theta=\theta_{0}}</script><script type="math/tex; mode=display">\nabla_{\theta} \eta\left(\pi_{\theta}\right)|_{\theta=\theta_{0}} = \sum_{s} \nabla_{\theta}\rho_{\pi}(s) \sum_{a}{\pi}(a | s) A_{\pi}(s, a)|_{\theta=\theta_{0}} + \sum_{s} \rho_{\pi}(s) \sum_{a} \nabla_{\theta}{\pi}(a | s) A_{\pi}(s, a)|_{\theta=\theta_{0}}</script><p>由于$\sum_{a}{\pi}(a | s) A_{\pi}(s, a)|_{\theta=\theta_{0}}=0$，因此第二个等式成立。</p>
<p>进一步，当策略更新满足如$\pi_{\text {new }}(a | s)=(1-\alpha) \pi_{\text {old }}(a | s)+\alpha \pi^{\prime}(a | s)$时，近似函数与原目标函数存在如下关系：</p>
<script type="math/tex; mode=display">
\eta\left(\pi_{\text {new }}\right) \geq L_{\pi_{\text {old }}}\left(\pi_{\text {new }}\right)-\frac{2 \epsilon \gamma}{(1-\gamma)^{2}} \alpha^{2}</script><p>或者用KL散度表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned} \eta(\tilde{\pi}) & \geq L_{\pi}(\tilde{\pi})-C D_{\mathrm{KL}}^{\max }(\pi, \tilde{\pi})  \\ & \text { where } C=\frac{2 \epsilon \gamma}{(1-\gamma)^{2}} \end{aligned}   \quad\quad\quad\quad (5)</script><p>公式(5)提供了$\eta(\tilde{\pi})$的下界，我们令$M_i(\pi)=L_{\pi_{i}}(\pi)-C D_{\mathrm{KL}}^{\max }\left(\pi_{i}, \pi\right)$，则有：</p>
<script type="math/tex; mode=display">
\begin{array}{l}{\eta\left(\pi_{i+1}\right) \geq M_{i}\left(\pi_{i+1}\right)} \\ {\eta\left(\pi_{i}\right)=M_{i}\left(\pi_{i}\right)} \\ {\eta\left(\pi_{i+1}\right)-\eta\left(\pi_{i}\right) \geq M_{i}\left(\pi_{i+1}\right)-M\left(\pi_{i}\right)}\end{array}</script><p>只要每次选择策略使得$M_i&gt;0$，则这样的策略就是更好的策略，算法就可以收敛到最优策略。</p>
<h3 id="优化问题的简化"><a href="#优化问题的简化" class="headerlink" title="优化问题的简化"></a>优化问题的简化</h3><p>从过前面的分析，我们把求解回报函数$\eta(\pi)$最大的问题，转化为求$\eta(\pi)$的下界函数$L_{\pi_{i}}(\pi)-C D_{\mathrm{KL}}^{\max }\left(\pi_{i}, \pi\right)$最大的问题，形式化为：</p>
<script type="math/tex; mode=display">
\underset{\theta}{\operatorname{maximize}}\left[L_{\theta_{\mathrm{old}}}(\theta)-C D_{\mathrm{KL}}^{\max }\left(\theta_{\mathrm{old}}, \theta\right)\right]</script><p>这个优化可以看做是在目标函数$L$的基础上，增加了一个惩罚项，即在最大化$L$的基础上，限制每次优化的参数变化程度，其中$C$为惩罚因子，KL散度表示参数之间的距离。但如果固定惩罚因子$C$，则每次迭代的步长很小，因此希望自动调整$C$的取值，这样优化问题可以转化为带约束优化的形式：</p>
<script type="math/tex; mode=display">
\begin{array}{l}{\underset{\theta}{\operatorname{maximize}} L_{\theta_{\text {old }}}(\theta)} \\ {\text { subject to } \bar{D}_{\mathrm{KL}}^{\rho_{\theta_{\text {old }}}}\left(\theta_{\text {old }}, \theta\right) \leq \delta}\end{array} \quad\quad\quad\quad (6)</script><p>进一步，我们将目标函数一阶逼近，在$\theta_{\text {old }}$进行一阶泰勒展开：</p>
<script type="math/tex; mode=display">L_{\theta_{\text {old }}}(\theta)\doteq \left.\nabla_{\theta} L_{\theta_{\text {old }}}(\theta)\right|_{\theta=\theta_{\text {old }}} \cdot\left(\theta-\theta_{\text {old }}\right)</script><p>将约束条件进行二阶逼近，在$\theta_{\text {old }}$进行二阶泰勒展开</p>
<script type="math/tex; mode=display">
\begin{aligned}
& D_{KL}(\pi_{\theta_{old}}(\cdot|s)||\pi_{\theta}(\cdot|s)) \\
& = E_{x\sim\pi_{old}}log\pi_{\theta_{old}} - E_{x\sim\pi_{old}}log\pi_{\theta} \\
& \approx E_{\theta_{old}}[log\pi_{\theta_{old}}] - (E_{\theta_{old}}[log\pi_{\theta_{old}}] + E_{\theta_{old}}[\nabla log\pi_{\theta_{old}}]\Delta\theta+\frac{1}{2}\Delta\theta^TE_{\theta_{old}}[\nabla^2 log\pi_{\theta_{old}}]\Delta\theta)\\
& = -\frac{1}{2}\Delta\theta^TE_{\theta_{old}}[\nabla^2 log\pi_{\theta_{old}}]\Delta\theta
\end{aligned}</script><p>其中最后一步的等式由于$log\pi_{\theta_{old}}$为极点，$\nabla log\pi_{\theta_{old}}=0$。</p>
<p>令$A = E_{\theta_{old}}[\nabla^2 log\pi_{\theta_{old}}]$，则优化问题最终可描述为：</p>
<script type="math/tex; mode=display">
\begin{array}{l}{\underset{\theta}{\operatorname{maximize}}\left[\left.\nabla_{\theta} L_{\theta_{\text {old }}}(\theta)\right|_{\theta=\theta_{\text {old }}} \cdot\left(\theta-\theta_{\text {old }}\right)\right]} \\ {\text { subject to } \frac{1}{2}\left(\theta_{\text {old }}-\theta\right)^{T} A\left(\theta_{\text {old }}\right)\left(\theta_{\text {old }}-\theta\right) \leq \delta}\end{array}</script><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>在梯度上升法中，我们判断最陡峭的方向，之后朝着那个方向前进。但是如果学习率太高的话，这样的优化也许会远离真正的最优策略。在TRPO中，我们用 δ 变量来限制我们的搜索区域，这样的区域可以保证在它达到局部或者全局最优策略之前，它的优化策略将会优于当前策略，从而使算法稳定的收敛到一个最优策略。</p>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://yoursite.com/2020/01/06/W7/" data-id="ck52j8r3z0005houxesnxb9ok" class="article-share-link">分享到</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-W6" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2020/01/06/W6/" class="article-date">
  <time datetime="2020-01-06T13:15:16.374Z" itemprop="datePublished">2020-01-06</time>
</h3>
    
  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/06/W6/">W6</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <p>最近做了一套基于IMU的人体姿态检测系统，用来取代动作捕捉系统。一方面IMU的实时性和可开发性比动捕要好，另一方面IMU成本很低的同时精度也足以满足需求。本文介绍IMU的基本原理与基于IMU的人体姿态检测系统的设计。</p>
<hr>
<h4 id="1-IMU的组成与原理"><a href="#1-IMU的组成与原理" class="headerlink" title="1.IMU的组成与原理"></a>1.IMU的组成与原理</h4><p><a href="https://imgchr.com/i/AzWyA1" target="_blank" rel="noopener"><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zMi5heDF4LmNvbS8yMDE5LzA0LzE3L0F6V3lBMS5wbmc?x-oss-process=image/format,png" alt="AzWyA1.png"></a></p>
<p>IMU可根据DOF的不同来加以区分，其中6DOF的IMU和9DOF的IMU比较常见，6DOF的IMU一般由一个三轴加速度计和一个三轴陀螺仪组成。9DOF的IMU会多一个三轴磁力计。模块所能感知状态量的个数即为DOF大小，DOF越多、精度越高的IMU也就越贵。<br>这里只讨论6DOF的IMU。</p>
<h6 id="加速度计"><a href="#加速度计" class="headerlink" title="加速度计"></a>加速度计</h6><hr>
<p>顾名思义，三轴加速度计能感受三轴的加速度，如上图中三个矢量所示。但要注意的是，当图示模块水平放置且静止，z轴方向加速度大小为<code>-g</code>，也就是说IMU测量的加速度与物体真实加速度相差一个重力加速度。<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zMi5heDF4LmNvbS8yMDE5LzA0LzE3L0F6VzI5Sy5wbmc?x-oss-process=image/format,png" alt="AzW29K.png">]<br>加速度计的物理实现是利用牛顿第二定律，如上图所示，中间红色物体为一个质量块，两头通过具有弹簧性质的长条结构与基底相连，红色的短栅与绿色的短栅分别为电容的极板。当基底在双箭头方向有加速度<code>a</code>时，由<code>f=ma=kx</code>，质量块会沿加速度相反的方向移动，红色极板与绿色极板之间的距离会发生变化，通过测量极板电容<code>C</code>的变化就可以得到加速度的大小。在三轴加速度计中，这样的结构在三个方向各有一个，且做到了微米的尺寸，并配合相应的测量电路集成在一个芯片中（如图一所示），构成一个微机电系统（MEMS）。</p>
<h6 id="陀螺仪"><a href="#陀螺仪" class="headerlink" title="陀螺仪"></a>陀螺仪</h6><hr>
<p>角速度测量的原理比加速度要复杂一些，因为涉及了科里奥里力(Coriolis Force)。所以我们先来说一下Coriolis Force。科里奥里力是由坐标系的转动与物体在动坐标系中的相对运动引起的，其本质是物体的惯性。</p>
<p> <img src="https://s2.ax1x.com/2019/04/17/AzbNVO.png" width="60%"></p>
<p>如果图示模块置于绝对静止的坐标系中，当在x方向施加一个驱动力使质量块运动时，根据牛顿第二定律，质量块只会在x方向上运动，而在y方向上不会运动。</p>
<p> <img src="https://s2.ax1x.com/2019/04/17/AzbUaD.png" width="60%"></p>
<p>但如果将图示模块置于一个旋转坐标系下，由于坐标系的旋转，使得当质量块沿x方向运动时，在y方向上会受到一个力，即科里奥里力<code>F=-2mvω</code>，从而使质量块沿y方向运动。地球上的很多自然现象，如热带气旋、季风带、河道两侧冲刷程度不同，都源于科里奥里力。</p>
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-6Gv6gntY-1575004767576)(<a href="https://s2.ax1x.com/2019/04/17/AzW6tx.png" target="_blank" rel="noopener">https://s2.ax1x.com/2019/04/17/AzW6tx.png</a>)]</p>
<p>陀螺仪的物理实现如上图所示，外侧的蓝色与黄色部分为驱动电极，内部的红色与蓝色为测量电极。在模块的驱动方向施加正弦驱动电压，当模块发生旋转时，质量块在垂直方向受科里奥里力影响也会产生一个正弦运动，且正弦运动的幅值与平台的角速度成正比，通过垂直方向的电极测量出此幅值，便可以得到模块角速度。与三轴加速度计一样，这样的结构在三轴陀螺仪的三个方向上各有一个，从而测量出三个方向的角速度。</p>
<h5 id="2-IMU姿态解算"><a href="#2-IMU姿态解算" class="headerlink" title="2. IMU姿态解算"></a>2. IMU姿态解算</h5><p>现在我们能够从IMU中得到三轴的加速度和三轴的角速度，接下来就要从这些数据中解算出三个方向的角度。实际上，单独由加速度计或陀螺仪都可以解算出三轴的角度。</p>
<h6 id="由加速度解算角度"><a href="#由加速度解算角度" class="headerlink" title="由加速度解算角度"></a>由加速度解算角度</h6><hr>
<p><a href="https://imgchr.com/i/AzjknI" target="_blank" rel="noopener"><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zMi5heDF4LmNvbS8yMDE5LzA0LzE3L0F6amtuSS5wbmc?x-oss-process=image/format,png" alt="AzjknI.png"></a></p>
<p>由于加速度计本质是测量力，所以在传感器静止的时候，测量的结果为重力加速度。当平台运动的加速度远小于重力加速度时，可认为传感器测量的结果全部为重力加速度，因此可以根据重力加速的在三轴分量的大小来解算出角度。</p>
<p><a href="https://imgchr.com/i/AzjijA" target="_blank" rel="noopener"><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zMi5heDF4LmNvbS8yMDE5LzA0LzE3L0F6amlqQS5wbmc?x-oss-process=image/format,png" alt="AzjijA.png"></a></p>
<p>以上图为例，传感器沿<code>y</code>方向倾斜的角度为：</p>
<script type="math/tex; mode=display">
Angle_{Accel} = arccos\frac{ax}{-g}</script><h6 id="由陀螺仪解算角度"><a href="#由陀螺仪解算角度" class="headerlink" title="由陀螺仪解算角度"></a>由陀螺仪解算角度</h6><hr>
<p>由角速度解算角度很容易理解，当知道陀螺仪的初始角度时，对角速度进行积分就可以得到角度值。</p>
<script type="math/tex; mode=display">
Angle_{Gyro}=Angle_0 + \int_0^t Gyro dt</script><h6 id="数据融合"><a href="#数据融合" class="headerlink" title="数据融合"></a>数据融合</h6><hr>
<p>现在我们可以从加速度和角速度分别解算出角度，但这两种方式都存在很大的问题。一方面由于加速度计容易受到振动的影响，噪声很大，所以解算角度的噪声也很大；另一方面虽然陀螺仪测量角速度的噪声不是很大，经过积分环节后噪声进一步被变小，但由于初始角度并不能准确得到，而且角速度存在零漂问题（即模块静止时角速度不完全为0，而是有一个偏置），经过积分后这个误差会被累积。<br>因此，两种方式解算出来的角度都无法直接使用，但我们可以采用数据融合的方法，把两种角度融合在一起，得到一个既没有累计误差、噪声又小的角度数据。</p>
<h6 id="互补滤波"><a href="#互补滤波" class="headerlink" title="互补滤波"></a>互补滤波</h6><hr>
<p>一阶互补滤波是最简单但却非常实用的数据融合算法，它把由加速度解算的角度和由角速度积分的角度按照一定比例加到一起，公式如下：</p>
<script type="math/tex; mode=display">
Angle = K\cdot Angle_{accel}+(1-K)\cdot (Angle + \omega\cdot dt)</script><p>其中参数<code>K</code>表示对加速度解算角度的置信程度，由于加速度的噪声很大，所以参数<code>K</code>一般很小，典型值为<code>0.05</code>，实际使用要根据效果来调整。</p>
<p>这样的互补滤波器可以看做是一个高通滤波和一个低通滤波的叠加：公式的第一项是为低通滤波部分，目的是滤除加速度的噪声；公式的第二项为高通滤波部分，目的是滤除角速度的直流偏置（零漂）。</p>
<h6 id="Kalman滤波"><a href="#Kalman滤波" class="headerlink" title="Kalman滤波"></a>Kalman滤波</h6><hr>
<p>互补滤波虽然简单实用易理解，但我们还有更好的算法来进行数据融合，比如Kalman滤波。关于Kalman滤波的原理，有一些非常好的论文和博客，感兴趣的同学可以移步这些网站:</p>
<p><a href="http://www.cs.unc.edu/~welch/kalman/media/pdf/Kalman1960.pdf" target="_blank" rel="noopener">Kalman先生1960年的论文</a><br><a href="https://list.yinxiang.com/markdown/eef42447-db3f-48ee-827b-1bb34c03eb83.php" target="_blank" rel="noopener">博客 How a Kalman filter works</a><br><a href="https://blog.csdn.net/u010720661/article/details/63253509" target="_blank" rel="noopener">上篇博客的中文版</a><br><a href="https://courses.engr.illinois.edu/ece420/sp2017/UnderstandingKalmanFilter.pdf" target="_blank" rel="noopener">论文 Understanding Kalman Filter</a><br><a href="https://www.jianshu.com/p/d3b1c3d307e0" target="_blank" rel="noopener">上篇论文的中文版</a></p>
<p>Kalman滤波的原理稍过复杂，这里并不打算深入讨论。但由于博客和论坛上关于用Kalman滤波融合加速度计和陀螺仪数据的建模方法介绍比较少，所以这里给出本问题状态空间方程的建立方法。</p>
<p>和一般系统的建模方法不同，本问题中的状态变量不是角度和角速度，而是角度和角速度的偏置：</p>
<script type="math/tex; mode=display">
State = \begin{bmatrix} Angle(k)  \\ Gyro_{bias}(k) \\  \end{bmatrix}</script><p>状态空间模型考虑带控制量的形式，其中控制量为陀螺仪测量的角速度，观测方程中只有从加速度解算得到的角度，方程如下：</p>
<script type="math/tex; mode=display">
\begin{bmatrix} Angle(k+1)  \\ Gyro_{bias}(k+1)   \end{bmatrix} = \begin{bmatrix} 1 & -dt \\ 0 & 1   \end{bmatrix}\begin{bmatrix} Angle(k)  \\ Gyro_{bias}(k) \end{bmatrix} + \begin{bmatrix} dt \\ 0  \end{bmatrix} Gyro(k) + W(k)</script><script type="math/tex; mode=display">
Angle_{accel}(k) =\begin{bmatrix} 1 & 0  \end{bmatrix}  \begin{bmatrix} Angle(k) \\ Gyro_{bias}(k)  \end{bmatrix} + V(k)</script><p>其中矩阵<code>W</code>和<code>V</code>分别为输入噪声矩阵和观测噪声矩阵，这两个矩阵的参数需要根据情况进行调整。对于本问题矩阵<code>V</code>的参数要比<code>W</code>的参数大很多，因为由加速度计观测角度的噪声很大。</p>
<hr>
<h5 id="3-使用IMU来测量关节角度"><a href="#3-使用IMU来测量关节角度" class="headerlink" title="3.使用IMU来测量关节角度"></a>3.使用IMU来测量关节角度</h5><p>这里基于IMU设计了一套用来测量人体姿态的硬件，通过在身体每个肢段上放置一个IMU，测量每段肢体在空间中的姿态角，下位机使用单片机通过串口接收每个IMU模块的数据，然后通过CAN总线发送给上位机，上位机再解算出关节角度。制作的模块如下：</p>
<p><img src="https://s2.ax1x.com/2019/04/18/ESLlLt.jpg" width="60%"><img src="https://s2.ax1x.com/2019/04/18/ESL3eP.jpg" width="33.6%"></p>
<p>下图是测量大腿和小腿的数据，Matlab对接收的数据进行解算并实时显示出来。</p>
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-9QTqCGKZ-1575004767580)(<a href="https://s2.ax1x.com/2019/04/18/ESXNKs.gif" target="_blank" rel="noopener">https://s2.ax1x.com/2019/04/18/ESXNKs.gif</a>)]</p>
<p>下图是由IMU得到的行走状态下肢三关节的角度变化。初次实验采样频率设置的有点低，只有50HZ，所以波形看起来不太平滑。</p>
<p><img src="https://s2.ax1x.com/2019/04/19/E9FtwF.png" width="90%"></p>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://yoursite.com/2020/01/06/W6/" data-id="ck52j8r410006houx9lbe1b9p" class="article-share-link">分享到</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-W4" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2020/01/06/W4/" class="article-date">
  <time datetime="2020-01-06T13:15:16.359Z" itemprop="datePublished">2020-01-06</time>
</h3>
    
  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/06/W4/">W4</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h1 id="数值计算基础"><a href="#数值计算基础" class="headerlink" title="数值计算基础"></a>数值计算基础</h1><h3 id="数据插值"><a href="#数据插值" class="headerlink" title="数据插值"></a>数据插值</h3><hr>
<p>数据插值可分为多项式（高次）插值、分段（低次）插值、三角插值等。多项式插值包括Lagrange插值、Aitken插值、Newton插值、Hermite插值，但高次插值会出现Runge现象，因此更多使用分段低次样条插值。<br>使用最多的为三次样条插值。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">yi = spline(x,y,xi)</span><br><span class="line">pp = spline(x,y)</span><br><span class="line">pp = csape(x,y,conds,valconds)</span><br><span class="line">    valconds: </span><br><span class="line">        'complete'          固定边界条件</span><br><span class="line">        'not-a-knot'        非扭结边界条件（默认）</span><br><span class="line">        'periodic'          周期边界条件</span><br><span class="line">        'second'            自由边界条件</span><br><span class="line">        'variational'       自然边界条件</span><br></pre></td></tr></table></figure>
<h3 id="数据拟合"><a href="#数据拟合" class="headerlink" title="数据拟合"></a>数据拟合</h3><hr>
<ul>
<li>工具箱：cftool + Code Generation</li>
<li>函  数：fit</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fitobject = fit(x,y,fitType)</span><br><span class="line">fitobject = fit([x,y],z,fitType)</span><br><span class="line">&lt;fitType -- cflibhelp&gt;</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>FitType</th>
<th>Equation</th>
</tr>
</thead>
<tbody>
<tr>
<td>poly1</td>
<td>Y = p1*x+p2</td>
</tr>
<tr>
<td>poly2</td>
<td>Y = p1<em>x^2+p2</em>x+p3</td>
</tr>
<tr>
<td>poly21</td>
<td>Z = p00 + p10<em>x + p01</em>y + p20<em>x^2 + p11</em>x*y</td>
</tr>
<tr>
<td>poly13</td>
<td>Z = p00 + p10<em>x + p01</em>y + p11<em>x</em>y + p02<em>y^2 + p12</em>x<em>y^2 + p03</em>y^3</td>
</tr>
<tr>
<td>weibull</td>
<td>Y = a<em>b</em>x^(b-1)<em>exp(-a</em>x^b)</td>
</tr>
<tr>
<td>exp1</td>
<td>Y = a<em>exp(b</em>x)</td>
</tr>
<tr>
<td>exp2</td>
<td>Y = a<em>exp(b</em>x)+c<em>exp(d</em>x)</td>
</tr>
<tr>
<td>fourier1</td>
<td>Y = a0+a1<em>cos(x</em>p)+b1<em>sin(x</em>p)</td>
</tr>
<tr>
<td>fourier2</td>
<td>Y = a0+a1<em>cos(x</em>p)+b1<em>sin(x</em>p)… +a2<em>cos(2</em>x<em>p)+b2</em>sin(2<em>x</em>p)</td>
</tr>
<tr>
<td>gauss1</td>
<td>Y = a1*exp(-((x-b1)/c1)^2)</td>
</tr>
<tr>
<td>gauss2</td>
<td>Y = a1<em>exp(-((x-b1)/c1)^2)+a2</em>… exp(-((x-b2)/c2)^2)</td>
</tr>
<tr>
<td>power1</td>
<td>Y = a*x^b</td>
</tr>
<tr>
<td>power2</td>
<td>Y = a*x^b+c</td>
</tr>
<tr>
<td>rat02</td>
<td>Y = (p1)/(x^2+q1*x+q2)</td>
</tr>
<tr>
<td>rat21</td>
<td>Y = (p1<em>x^2+p2</em>x+p3)/(x+q1)</td>
</tr>
<tr>
<td>sin1</td>
<td>Y = a1<em>sin(b1</em>x+c1)</td>
</tr>
<tr>
<td>sin2</td>
<td>Y = a1<em>sin(b1</em>x+c1)+a2<em>sin… (b2</em>x+c2)</td>
</tr>
</tbody>
</table>
</div>
<h3 id="数值积分"><a href="#数值积分" class="headerlink" title="数值积分"></a>数值积分</h3><hr>
<div class="table-container">
<table>
<thead>
<tr>
<th>函数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>int</td>
<td>符号函数</td>
</tr>
<tr>
<td>trapz</td>
<td>非函数表达式梯形积分</td>
</tr>
<tr>
<td>quad</td>
<td>自适应Simpson积分，对非光滑函数最为有效的地低阶积分</td>
</tr>
<tr>
<td>quadl</td>
<td>适用于光滑函数，在高精度时比quad更有效</td>
</tr>
<tr>
<td>quadgk</td>
<td>对震荡的被积函数最有效，支持无限区间积分，允许积分区间短点有若的奇异性</td>
</tr>
<tr>
<td>quadv</td>
<td>quad积分函数的向量化版本</td>
</tr>
<tr>
<td>quad2d</td>
<td>平面区域的二重积分，为通用的二重积分，可以做变上限、下限积分</td>
</tr>
<tr>
<td>dblquad</td>
<td>矩形区域的二重积分</td>
</tr>
<tr>
<td>triplequad</td>
<td>长方体区域的三重积分</td>
</tr>
</tbody>
</table>
</div>
<h3 id="常微分方程"><a href="#常微分方程" class="headerlink" title="常微分方程"></a>常微分方程</h3><hr>
<p>常微分方程求解器比较<br>|求解器|刚性|算法|应用差场合|<br>|—-   |—- |—- |—-       |<br>|ode45|非刚性|单步，4-5阶龙格-库塔|首选，大多数场合|<br>|ode23|非刚性|单步，2-3阶龙格-库塔|精度较低场合|<br>|ode113|非刚性|多步，1-13阶Adams算法|若ode45计算时间长，尝试ode113|<br>|ode15s|刚性|多步，Gear’s反向数值积分|存在质量矩阵时|<br>|ode23s|刚性|单步，2阶Rosenbrock算法|低精度或存在质量矩阵时|<br>|ode23t|中等刚性|梯形算法|中等刚度问题|<br>|ode23tb|刚性|梯形-反向数值微分两阶段算法|低精度或存在质量矩阵|<br>|ode15i|完全隐式|可以求解完全隐式的常微分方程|完全隐式方程|</p>
<p>使用odefile.m模板求解常微分方程</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">function varargout = odefile(t,y,flag,p1,p2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">switch</span> flag</span><br><span class="line"><span class="keyword">case</span> <span class="string">''</span></span><br><span class="line">    varargout&#123;1&#125; = f(t,y,p1,p2);</span><br><span class="line">case 'init'</span><br><span class="line">    [varargout&#123;<span class="number">1</span>:<span class="number">3</span>&#125;] = init(p1,p2);</span><br><span class="line">case 'jacobian'</span><br><span class="line">    varargout&#123;<span class="number">1</span>&#125; = jacobian(t,y,p1,p2);</span><br><span class="line">case 'jpattern'</span><br><span class="line">    varargout&#123;<span class="number">1</span>&#125; = jpattern(t,y,p1,p2);</span><br><span class="line">case 'mass'</span><br><span class="line">    varargout&#123;<span class="number">1</span>&#125; = mass(t,y,p1,p2);</span><br><span class="line">case 'events'</span><br><span class="line">    [varargout&#123;<span class="number">1</span>:<span class="number">3</span>&#125;] = events(t,y,p1,p2);</span><br><span class="line">otherwise</span><br><span class="line">    error['Unknown flag'];</span><br><span class="line"><span class="built_in">end</span></span><br><span class="line"></span><br><span class="line">function dydt = f(t,y,p1,p2)</span><br><span class="line">    dydt = &lt;在此插入常微分方程&gt;;</span><br><span class="line"></span><br><span class="line">function [tspan,y0,options] = init(p1,p2)</span><br><span class="line">    tspan = &lt;在此插入求解范围&gt;;</span><br><span class="line">    y0 = &lt;在此插入初值&gt;;</span><br><span class="line">    options = &lt;在此插入选项设置，可设置为[]&gt;;</span><br><span class="line"></span><br><span class="line">function dfdy = jacobian(t,y,p1,p2)</span><br><span class="line">    dfdy = &lt;再此插入雅克比矩阵&gt;;</span><br><span class="line"></span><br><span class="line">function S = jpattern(t,y,p1,p2)</span><br><span class="line">    S = &lt;在此插入雅克比矩阵系数模式&gt;;</span><br><span class="line"></span><br><span class="line">function M = mass(t,y,p1,p2)</span><br><span class="line">    M = &lt;在此插入质量矩阵&gt;;</span><br><span class="line"></span><br><span class="line">function [value,isterminal,direction] = events(t,y,p1,p2)</span><br><span class="line">    value = &lt;在此插入时间向量函数&gt;;</span><br><span class="line">    isterminal = &lt;在此插入逻辑向量&gt;;</span><br><span class="line">    direction = &lt;在此插入方向向量&gt;;</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://yoursite.com/2020/01/06/W4/" data-id="ck52j8r3v0004houxb54udqqw" class="article-share-link">分享到</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-W3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2020/01/06/W3/" class="article-date">
  <time datetime="2020-01-06T13:15:16.352Z" itemprop="datePublished">2020-01-06</time>
</h3>
    
  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/06/W3/">W3</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="力矩参数扫描实验"><a href="#力矩参数扫描实验" class="headerlink" title="力矩参数扫描实验"></a>力矩参数扫描实验</h2><p>为了进一步理解人如何适应外骨骼助力，我们进行了一系列助力参数扫描实验，研究在助力参数连续改变的过程中，人体运动学、动力学、与生理的变化趋势。在这一系列实验中，我们改变的参数主要为峰值力矩（$20N\cdot m\to60N\cdot m$）和峰值时间（$44\%\to52\%$）</p>
<table>
    <tr>
        <td><center>
        <img src="https://img-blog.csdnimg.cn/20191020222547375.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDA3NTQw,size_16,color_FFFFFF,t_70" />
        </center>
        <center>
        图1 期望峰值力矩参数连续变化
        </center></td> 
    <tr>
</table>

<h3 id="运动学变化"><a href="#运动学变化" class="headerlink" title="运动学变化"></a>运动学变化</h3><p>图2所示的为峰值力矩增大过程(峰值时间为48%)中踝关节角度的变化，在峰值较小时踝关节角度在40%-50%步态周期之间存在减小，即背屈运动趋势；但在峰值力矩较大时，这种趋势消失并转变为跖屈运动。从步态循环的角度，在支撑中期踝关节存在背屈运动阶段，使得整条腿绕着踝关节转动（而非绕前脚掌），这样可以减小重心在竖直方向上的变化（相较与绕前脚掌的旋转半径更小），并减小踝关节伸肌的作用。这种改变表明，支撑中期时步态过程被助力影响，这种影响会导致质心竖直方向的位移变大、两侧步态过程不对称。</p>
<table>
    <tr>
        <td><center>
        <img src="https://img-blog.csdnimg.cn/201910202227488.bmp?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDA3NTQw,size_16,color_FFFFFF,t_70" />
        </center>
        <center>
        图2 峰值力矩增加过程中踝关节角度的变化
        </center></td> 
    <tr>
</table>

<p>图3所示的为峰值时间增大过程（峰值力矩为$35N\cdot m$）中踝关节角度的变化，与图2具有类似的现象，支撑中期踝关节角度受到了助力参数变化的影响，这影响在峰值时间较小时更明显，而在峰值时间较大时（52%）更接近自然步态的踝关节角度，但52%的峰值时间或许是一个较为舒适的时间，再增加或许会出现相反的变化趋势。</p>
<table>
    <tr>
        <td><center>
        <img src="https://img-blog.csdnimg.cn/20191020223154712.bmp?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDA3NTQw,size_16,color_FFFFFF,t_70" />
        </center>
        <center>
        图3 峰值时间增加过程中踝关节角度的变化
        </center></td> 
    <tr>
</table>

<p>图2与图3反应的另外一个问题是：力矩参数的改变，对于步态过程中运动学的影响主要在于支撑中期，对支撑末期的改变较小，踝关节角度的峰值与峰值时刻基本没有发生明显变化。这说明人体本身的行走模式是很难改变的，这涉及了步态过程运动学动力学的连续性与人体生理结构的内在限制。另一方面或许存在着合适的力矩曲线形式能够对支撑末期产生更大的影响。</p>
<h3 id="机械功率变化"><a href="#机械功率变化" class="headerlink" title="机械功率变化"></a>机械功率变化</h3><p>图4与图5反应的是在峰值力矩和峰值时间变化的过程中，踝关节外骨骼机械功率的变化。图4反应在峰值力矩较小时，外骨骼在大部分时间做的是负功，负功的出现是由于支撑中期踝关节的背屈运动，导致力矩方向与运动方向相反；当力矩逐渐增大时，负功逐渐减小，支撑末期的功率峰值逐渐增加，符合人自然行走时踝关节的功率曲线。<br>踝关节的负功率是可以理解的，当力矩较小时人体有能力抵抗电机的拉力，而当力矩较大时则没办法抵抗。消失的负功并不能反映出助力效果的增加，也可以理解为助力模式并不符合人体的期望。</p>
<table>
    <tr>
        <td><center>
        <img src="https://img-blog.csdnimg.cn/20191020231054135.bmp?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDA3NTQw,size_16,color_FFFFFF,t_70" />
        </center>
        <center>
        图4 峰值时间增加过程中踝关节机械功率的变化
        </center></td> 
    <tr>
</table>
图5表明峰值时间变化时会改变踝关节的机械功率，这种改变是预料之内的，但改变程度之大却是在预料之外的。从数值上看，峰值时间对于功率曲线的改变甚至比峰值力矩参数还要大。较大的机械功率不能一定表明为人体提供了更多的辅助，但较小的机械功率可以说明没有为人体提供合适的辅助。在峰值力矩适中时，人对于助力的峰值时间比峰值力矩要更加敏感。

<table>
    <tr>
        <td><center>
        <img src="https://img-blog.csdnimg.cn/20191020231146303.bmp?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDA3NTQw,size_16,color_FFFFFF,t_70" />
        </center>
        <center>
        图5 峰值时间增加过程中踝关节机械功率的变化
        </center></td> 
    <tr>
</table>

<p>在峰值力矩增加的过程中，人体会逐渐适应力矩的辅助；当力矩再次下降时，人体可能会由于适应的原因，对于相同峰值力矩表现出不同的反应。因此在力矩增加减小的过程中，一些特征可能会出现迟滞现象。在图6所示的机械功曲线中，这种迟滞现象是存在的，并在峰值力矩逐渐减小过程中逐渐明显。</p>
<table>
    <tr>
        <td><center>
        <img src="https://img-blog.csdnimg.cn/2019102114450358.bmp?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDA3NTQw,size_16,color_FFFFFF,t_70" />
        </center>
        <center>
        图6 峰值力矩增加和减少过程中外骨骼做功的变化
        </center></td> 
    <tr>
</table>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://yoursite.com/2020/01/06/W3/" data-id="ck52j8r3q0002houxd3r39yyt" class="article-share-link">分享到</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-W2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2020/01/06/W2/" class="article-date">
  <time datetime="2020-01-06T13:15:16.342Z" itemprop="datePublished">2020-01-06</time>
</h3>
    
  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/06/W2/">W2</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h3 id="混合控制中的平滑切换问题"><a href="#混合控制中的平滑切换问题" class="headerlink" title="混合控制中的平滑切换问题"></a>混合控制中的平滑切换问题</h3><p>当前的控制策略采取支撑相力矩控制、摆动相角度跟随的混合控制策略，这种策略下的一个重要的问题是如何在两种控制之间进行切换。之前算法中直接在步态周期的65\%时刻直接切换控制模式，这种方式会导致切换瞬间控制器由于过大的误差而产生过大的控制输出，从而使系统存在不稳定的风险。</p>
<p>如图1所示，在步态状态切换时（65\%的步态周期），由于角度跟踪误差，控制器会产生一个较大控制输出，使得电机控制指令会经历一个跳变。这种跳变的控制信号对于电机而言是不利的，它会降低电机的使用寿命，更极端的会使电机因超过其极限性能而出错。</p>
<p>在这里我们采用一个非常朴素的解决方法：在两种模式切换时，以滑动比例的方式平滑过渡。在期望力矩下降到0（或者在步态周期达到60%时）进入模式过渡阶段，两种控制策略（力矩与跟随）同时作用，但各自的权重逐渐变化，直到完全变为跟随控制，接着进入摆动阶段跟随控制，即$vel_{cmd}=(1-\alpha)\cdot vel_{torque}+\alpha\cdot vel_{follow}$。经过平滑过渡后的电机控制指令如图3所示，信号跳变问题得到了极大的改善。</p>
<table>
    <tr>
        <td><center>
        <img src="https://img-blog.csdnimg.cn/20191020220046816.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDA3NTQw,size_16,color_FFFFFF,t_70" />
        </center>
        <center>
        图1 电机控制指令关于步态周期的曲线
        </center></td> 
    <tr>
</table>

<table>
    <tr>
        <td><center>
        <img src="https://img-blog.csdnimg.cn/2019102022070124.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDA3NTQw,size_16,color_FFFFFF,t_70" />
        </center>
        <center>
        图2 加入过渡阶段的控制时序
        </center></td> 
    <tr>
</table>

<table>
    <tr>
        <td><center>
        <img src="https://img-blog.csdnimg.cn/20191020220136408.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDA3NTQw,size_16,color_FFFFFF,t_70" />
        </center>
        <center>
        图3 平滑切换策略下，电机控制指令关于步态周期的曲线
        </center></td> 
    <tr>
</table>

<h3 id="助力过程鲍登线的长度变化"><a href="#助力过程鲍登线的长度变化" class="headerlink" title="助力过程鲍登线的长度变化"></a>助力过程鲍登线的长度变化</h3><p>在安装踝关节编码器后，可以基于踝关节角度和电机角度对鲍登线-弹簧的长度变化和刚度系数进行分析。<br>首先基于力矩控制实验的数据，我们计算了在峰值力矩为$35N\cdot m$时，鲍登线（弹簧）长度的变化（$\theta_m - \theta_aR$）与测量力矩的关系曲线，如图4所示，其中彩色曲线为60步的平均曲线，蓝色端表示步态开始，黄色端表示步态结束。由图可以看出，尽管曲线存在明显的非线性（死区：长度变化较小时力矩没有变化的问题，迟滞：力矩上升和下降时曲线不一致），但这些非线性似乎是确定的，而且鲍登线与弹簧整体的刚度系数（曲线斜率）似乎保是一个常数。若这种现象在力矩变化的时候也成立，或许可以考虑对非线性过程建模，从而设计更复杂的控制器。</p>
<table>
    <tr>
        <td><center>
        <img src="https://img-blog.csdnimg.cn/20191020221102842.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDA3NTQw,size_16,color_FFFFFF,t_70" />
        </center>
        <center>
        图4 峰值力矩为$35N\cdot m$时，等效弹簧长度的变化与交互力矩的关系
        </center></td> 
    <tr>
</table>

<p>之后我们进行了进一步的实验，将峰值力矩从为$35N\cdot m$逐渐增加到$60N\cdot m$。在变化的过程中，鲍登线（弹簧）长度的变化与交互力矩的关系如图1.13所示，其中蓝色曲线代表峰值力矩较小的情况，黄色曲线代表峰值力矩较大的情况。在这个实验中，虽然鲍登线与弹簧整体的刚度系数仍保持着固定常数关系，但非线性情况发生了明显变化:随着峰值力矩的增加，曲线整体向右移动。这或许是由于较大的峰值力矩导致鲍登线整体被拉长，使得非线性的死区阈值发生了变化。图1.14,鲍登线（弹簧）长度变化关于步态周期的曲线也能够佐证这一观点，在较大的拉力作用下，鲍登线整体被拉长了约1cm。<br>此实验结果强调了对鲍登绳进行最大力矩完全拉伸实验的重要性。关于鲍登绳完全拉伸后的长度与交互力矩关系，后续实验会继续进行研究。</p>
<table>
    <tr>
        <td><center>
        <img src="https://img-blog.csdnimg.cn/20191020221353229.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDA3NTQw,size_16,color_FFFFFF,t_70" />
        </center>
        <center>
        图5 峰值力矩变化时，等效弹簧长度的变化与交互力矩的关系
        </center></td> 
    <tr>
</table>


      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://yoursite.com/2020/01/06/W2/" data-id="ck52j8r3s0003houx8wjv18yc" class="article-share-link">分享到</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-W1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2020/01/06/W1/" class="article-date">
  <time datetime="2020-01-06T13:15:16.333Z" itemprop="datePublished">2020-01-06</time>
</h3>
    
  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/06/W1/">W1</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h3 id="1-外骨骼硬件优化"><a href="#1-外骨骼硬件优化" class="headerlink" title="1.外骨骼硬件优化"></a>1.外骨骼硬件优化</h3><p>对外骨骼的传感系统进行若干改进尝试。</p>
<h4 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h4><p>为外骨骼更换一体式光栅编码器，该编码器精度高（分辨率2000线，最高3600线），价格低（200元），安装方便，受外界干扰小，不会受到外骨骼晃动的影响，不会受到接线不良的影响。建议下一版外骨骼设计时采用磁编码器。<br>设计了编码器装配外骨骼的3D打印件。由于外骨骼上没有安装孔，所以目前只能用过强力胶将其粘在外骨骼结构上，无法拆卸。</p>
<table>
    <tr>
        <td><center>
        <img src="https://img-blog.csdnimg.cn/20191020210908161.bmp?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDA3NTQw,size_16,color_FFFFFF,t_70" />
        </center>
        <center>
        图1 一体式光栅编码器
        </center></td> 
    <tr>
</table>

<h4 id="足底开关"><a href="#足底开关" class="headerlink" title="足底开关"></a>足底开关</h4><p>尝试采用触碰开关（图2）代替之前的足底开关来实现步态检测。</p>
<ul>
<li>优点：价格较低（1元/个）,信号无毛刺问题（图1.3），降低步态检测出错概率。</li>
<li>缺点：安装在鞋子外部，可能因以为碰撞而损坏；需要设计合适的安装方式。</li>
</ul>
<table>
    <tr>
        <td><center>
        <img src="https://img-blog.csdnimg.cn/20191020212132892.bmp?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDA3NTQw,size_16,color_FFFFFF,t_70" />
        </center>
        <center>
        图2 触碰开关
        </center></td> 
    <tr>
</table>

<h4 id="拉力传感器"><a href="#拉力传感器" class="headerlink" title="拉力传感器"></a>拉力传感器</h4><p> 踝关节外骨骼硬件结构中最复杂、最昂贵是钛合金悬臂结构，此构件最大的作用是通过测量金属结构上的应变来计算交互力矩。若能够有更简单的力（矩）测量方式，则可以极大的降低加工成本。</p>
<ul>
<li>优点：价格低（300元/个）,安装容易（设计合理安装结构即可，无需应变片粘贴），信号受噪声干扰小（初步感受，未进行定量比较）。</li>
<li>缺点：体积略大，在现有外骨骼结构上无法与串联弹簧同时安装。</li>
</ul>
<table>
    <tr>
        <td><center>
        <img src="https://img-blog.csdnimg.cn/20191020211811859.bmp?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDA3NTQw,size_16,color_FFFFFF,t_70" />
        </center>
        <center>
        图3 拉力传感器
        </center></td> 
    <tr>
</table>

<h3 id="2-外骨骼控制测试"><a href="#2-外骨骼控制测试" class="headerlink" title="2.外骨骼控制测试"></a>2.外骨骼控制测试</h3><p>对踝关节外骨骼进行了若干控制测试。</p>
<h4 id="跟随控制"><a href="#跟随控制" class="headerlink" title="跟随控制"></a>跟随控制</h4><p>使用踝关节编码器角度作为反馈，进行角度跟随控制。实验中发现，单纯基于比例控制响应速度略慢，因此又增加了角速度反馈。除此之外，电机与踝关节的传动比$R$，以及角度跟随的宽松偏置也在实验中进行调整，调整结果为$R=40/108$，踝关节宽松角度为$1\deg$。为了确定跟随模式的频率特性，分别测试了行走（1.25m/s）和跑步（1.95m/s）模式下的角度跟随效果。在行走和跑步时，外骨骼角度跟随控制均有较好的效果。行走时的被动力矩在$3N\cdot m$内，跑步时的被动力矩在$5N\cdot m$内。</p>
<table>
    <tr>
        <td ><center><img src="https://img-blog.csdnimg.cn/20191020212838674.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDA3NTQw,size_16,color_FFFFFF,t_70" > 图4 跟随模式下行走时踝关节的角度变化 </center></td>
        <td ><center><img src="https://img-blog.csdnimg.cn/20191020212945554.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDA3NTQw,size_16,color_FFFFFF,t_70"  >图5 跟随模式下行走时踝关节被动力矩</center></td>
    </tr>
     <tr>
        <td ><center><img src="https://img-blog.csdnimg.cn/20191020213202108.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDA3NTQw,size_16,color_FFFFFF,t_70" > 图6 跟随模式下跑步时踝关节的角度变化 </center></td>
        <td ><center><img src="https://img-blog.csdnimg.cn/20191020213217210.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDA3NTQw,size_16,color_FFFFFF,t_70"  >图7 跟随模式下跑步时踝关节被动力矩</center></td>
    </tr>
</table>
#### 跟踪精度测试
在将跟随控制作为摆动阶段的控制策略后，对混合控制（支撑相-力矩控制，摆动相-跟随控制）的力矩跟踪精度进行了测试，以确定当前外骨骼系统力矩跟踪的极限水平。

- 目的：测试外骨骼系统在不同控制器下的力矩跟踪的极限精度
- 实验条件：峰值力矩$35N\cdot m$，峰值时间48\%
- 控制算法与控制参数：PD（Kp=4，Kd=-0.02），PD+LRN（Kp=3，Kd=-0.01，Kl=0.15）
- 实验结果：PD-RMSE=1.61（4.6\%），PD+LRN-RMSE=0.74（2.1\%）

<table>
    <tr>
        <td><center>
        <img src="https://img-blog.csdnimg.cn/20191020214333157.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDA3NTQw,size_16,color_FFFFFF,t_70" />
        </center>
        <center>
        图8 PD控制下的力矩跟踪结果
        </center></td> 
    <tr>
</table>

<table>
    <tr>
        <td><center>
        <img src="https://img-blog.csdnimg.cn/20191020214451167.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDA3NTQw,size_16,color_FFFFFF,t_70" />
        </center>
        <center>
        图9 PD+LRN控制下的力矩跟踪结果
        </center></td> 
    <tr>
</table>

<h4 id="迭代学习的收敛性"><a href="#迭代学习的收敛性" class="headerlink" title="迭代学习的收敛性"></a>迭代学习的收敛性</h4><p>力矩控制实验中我们发现，对于相同的迭代学习参数，PD反馈控制器的参数会影响迭代学习的收敛精度和收敛速度。图10展示开启迭代学习后力矩跟踪误差的下降曲线。额外实验表明，较大的控制参数可能使得系统震荡，或有震荡的趋势，使得迭代学习的误差信号中包含很多扰动，从而降低了学习效率和学习精度。同时曲线表明，所示的参数下，迭代学习可以在50步左右学习稳定。</p>
<table>
    <tr>
        <td><center>
        <img src="https://img-blog.csdnimg.cn/20191020214728913.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDA3NTQw,size_16,color_FFFFFF,t_70" />
        </center>
        <center>
        图10 迭代学习过程的误差下降曲线
        </center></td> 
    <tr>
</table>


      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://yoursite.com/2020/01/06/W1/" data-id="ck52j8r310000houxejt2fsfx" class="article-share-link">分享到</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
</section>
      
      <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">近期文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/01/06/TRPO/">TRPO</a>
          </li>
        
          <li>
            <a href="/2020/01/06/DDPG/">DDPG</a>
          </li>
        
          <li>
            <a href="/2020/01/06/W7/">W7</a>
          </li>
        
          <li>
            <a href="/2020/01/06/W6/">W6</a>
          </li>
        
          <li>
            <a href="/2020/01/06/W4/">W4</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a><span class="archive-list-count">8</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 JYChen<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
  <a href="#search" class="mobile-nav-link st-search-show-outputs">搜索</a>
</nav>

  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    menuSettings: {
      zoom: "None"
    },
    showMathMenu: false,
    jax: ["input/TeX","output/CommonHTML"],
    extensions: ["tex2jax.js"],
    TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js"],
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
  });
</script>

<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.1/MathJax.js"></script>



<!-- totop start -->
<div id="totop">
	<a title="返回顶部"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


</div>
</body>
</html>
